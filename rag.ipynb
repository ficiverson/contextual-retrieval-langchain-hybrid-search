{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade \"langchain>=0.1.0\" \"pydantic>=2.0.0\" rank_bm25 PyPDF2 ragas bert-score pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "llm_model = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4.1\",\n",
    "    model_version=\"2025-04-14\",\n",
    "    api_key=\"*******\",\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=\"*********\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings( \n",
    "    api_key=\"******\",\n",
    "    azure_endpoint=\"**************\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    dimensions=1024\n",
    "   )\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "full_documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert PDF into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def pdf_to_chunks_json(\n",
    "    pdf_path: str,\n",
    "    file_url: str,\n",
    "    id: str,\n",
    "    output_json: str = None\n",
    "):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    content = []\n",
    "    for page_num, page in enumerate(tqdm(reader.pages, desc=\"Reading PDF\")):\n",
    "        text = page.extract_text() or \"\"\n",
    "        if text.strip():\n",
    "            content.append({\n",
    "                \"chunks\": [{\n",
    "                    \"text\": text.strip(),\n",
    "                    \"metadata\": {\"page\": page_num + 1}\n",
    "                }]\n",
    "            })\n",
    "\n",
    "    result = [{\n",
    "        \"file\": file_url,\n",
    "        \"id\": id,\n",
    "        \"content\": content\n",
    "    }]\n",
    "\n",
    "    if output_json:\n",
    "        with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "pdf_to_chunks_json(\n",
    "    pdf_path=\"pdf/memoria22.pdf\",\n",
    "    file_url=\"https://cuacfm.org\",\n",
    "    id=\"memoria22\",\n",
    "    output_json=\"chunks/memoria22.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store chunks into vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON\n",
    "with open('chunks/memoria22.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "chunks = []\n",
    "texts = []\n",
    "for entry in data:\n",
    "    for content in entry.get('content', []):\n",
    "        for chunk in content.get('chunks', []):\n",
    "            text = chunk['text']\n",
    "            metadata = chunk.get('metadata', {})\n",
    "            chunks.append((text, metadata))\n",
    "            texts.append(text)\n",
    "\n",
    "# Prepare BM25\n",
    "tokenized_corpus = [text.split() for text in texts]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Define the prompt template for the chain\n",
    "template = \"\"\"\n",
    "<document>\n",
    "{doc_content}\n",
    "</document>\n",
    "\n",
    "Here is the chunk we want to situate within the whole document\n",
    "<chunk>\n",
    "{chunk_content}\n",
    "</chunk>\n",
    "\n",
    "Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "Answer only with the succinct context and nothing else.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"doc_content\", \"chunk_content\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain = prompt | llm_model\n",
    "\n",
    "doc_content = \"\\n\".join(texts)\n",
    "documents = []\n",
    "\n",
    "for i, (text, metadata) in enumerate(chunks):\n",
    "    # Run the chain to get the context    \n",
    "    context = chain.invoke({\"doc_content\": doc_content, \"chunk_content\": text})\n",
    "    print(context)\n",
    "    if hasattr(context, \"content\"):\n",
    "        context = context.content\n",
    "    metadata['situated_context'] = context\n",
    "    metadata['original_context'] = chunk\n",
    "    scores = bm25.get_scores(text.split())\n",
    "    metadata['bm25_score'] = float(scores[i])\n",
    "    document = Document(page_content=text, metadata=metadata)\n",
    "    documents.append(document)\n",
    "    full_documents.append(document)\n",
    "\n",
    "vector_store.add_documents(documents=documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 100\n",
    "for index, (id, doc) in enumerate(vector_store.store.items()):\n",
    "    if index < top_n:\n",
    "        # docs have keys 'id', 'vector', 'text', 'metadata'\n",
    "        print(f\"{doc['metadata']} --> {doc['metadata']}\")\n",
    "    else:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the model to answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for the chain\n",
    "template = \"\"\"\n",
    "You are a helpful assistant that can answer questions about CUAC FM. Here is the context:\n",
    "{context} and here the question:\n",
    "{query}\n",
    "Mention the page number and name of the document of the context where the answer is found at the end of the answer.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template=template\n",
    ")\n",
    "chain = prompt | llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare retrievers: bm25, vector and hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "def retriever_hybrid(query):\n",
    "    # BM25 retriever\n",
    "    bm25_retriever = BM25Retriever.from_documents(full_documents)\n",
    "    bm25_retriever.k = 20\n",
    "\n",
    "    # Vector retriever from in-memory vector store\n",
    "    vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 20},search_type=\"similarity\")\n",
    "    # Ensemble retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, vector_retriever],\n",
    "        weights=[0.4, 0.6]\n",
    "    )\n",
    "    hybrid_result = ensemble_retriever.get_relevant_documents(query)\n",
    "    return hybrid_result\n",
    "\n",
    "    \n",
    "def retriever_bm25(query):\n",
    "    bm25_retriever = BM25Retriever.from_documents(full_documents)\n",
    "    bm25_retriever.k = 20\n",
    "    return bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "def retriever_vector(query):\n",
    "    vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 20},search_type=\"similarity\")\n",
    "    return vector_retriever.get_relevant_documents(query)\n",
    "\n",
    "def pretty_print_documents(docs, max_docs=5):\n",
    "    for i, doc in enumerate(docs[:max_docs]):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"Text: {doc.page_content[:300]}...\")  # Print first 300 chars\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "\n",
    "query = \"¬øQu√© es CUAC FM?\"\n",
    "\n",
    "result = chain.invoke({\"context\": retriever_hybrid(query), \"query\": query})\n",
    "print(result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data frame to evaluate with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Example query\n",
    "cuac_fm_questions = [\n",
    "    # üìª Preguntas generales sobre CUAC FM\n",
    "    \"¬øQu√© es CUAC FM?\",\n",
    "    \"¬øCu√°ndo y c√≥mo naci√≥ CUAC FM?\",\n",
    "    \"¬øQu√© tipo de radio es CUAC FM?\",\n",
    "    \"¬øD√≥nde tiene su sede CUAC FM?\",\n",
    "    \"¬øQu√© significa CUAC?\",\n",
    "\n",
    "    # üßë‚Äçü§ù‚Äçüßë Participaci√≥n y organizaci√≥n\n",
    "    \"¬øQui√©n puede participar en CUAC FM?\",\n",
    "    \"¬øC√≥mo se organizan los programas en CUAC FM?\",\n",
    "    \"¬øQu√© papel tiene el voluntariado en CUAC FM?\",\n",
    "    \"¬øQu√© formaci√≥n ofrece CUAC FM a los nuevos colaboradores?\",\n",
    "    \"¬øC√≥mo se puede proponer un nuevo programa en CUAC FM?\",\n",
    "\n",
    "    # üó£Ô∏è Contenido y programaci√≥n\n",
    "    \"¬øQu√© tipo de programas se emiten en CUAC FM?\",\n",
    "    \"¬øQu√© temas suelen tratarse en la emisora?\",\n",
    "    \"¬øC√≥mo se selecciona la programaci√≥n de CUAC FM?\",\n",
    "    \"¬øHay espacio para la m√∫sica local y emergente en CUAC FM?\",\n",
    "    \"¬øCUAC FM tiene una programaci√≥n estable o cambia a menudo?\",\n",
    "\n",
    "    # üéß Tecnolog√≠a y difusi√≥n\n",
    "    \"¬øEn qu√© frecuencia emite CUAC FM?\",\n",
    "    \"¬øSe puede escuchar CUAC FM por internet?\",\n",
    "    \"¬øQu√© tipo de tecnolog√≠a usa CUAC FM para grabar y emitir programas?\",\n",
    "    \"¬øLos programas est√°n disponibles en formato podcast?\",\n",
    "    \"¬øD√≥nde se pueden encontrar los programas antiguos?\",\n",
    "\n",
    "    # üì¢ Compromiso social y legal\n",
    "    \"¬øQu√© papel juega CUAC FM como medio comunitario?\",\n",
    "    \"¬øQu√© derechos y deberes tiene CUAC FM seg√∫n la legislaci√≥n espa√±ola?\",\n",
    "    \"¬øCUAC FM ha tenido conflictos o problemas legales por su licencia de emisi√≥n?\",\n",
    "    \"¬øC√≥mo se financia CUAC FM?\",\n",
    "    \"¬øQu√© relaci√≥n tiene CUAC FM con la Red de Medios Comunitarios (ReMC)?\",\n",
    "\n",
    "    # üåç Impacto y proyecci√≥n\n",
    "    \"¬øQu√© impacto tiene CUAC FM en la comunidad local?\",\n",
    "    \"¬øQu√© actividades realiza CUAC FM fuera de la radio (talleres, eventos, colaboraciones)?\",\n",
    "    \"¬øHa recibido CUAC FM alg√∫n reconocimiento o premio?\",\n",
    "    \"¬øCon qu√© otras radios o entidades colabora CUAC FM?\",\n",
    "    \"¬øQu√© desaf√≠os enfrenta actualmente CUAC FM?\"\n",
    "]\n",
    "\n",
    "\n",
    "cuac_fm_answers = [\n",
    "    # üìª Preguntas generales sobre CUAC FM\n",
    "    \"CUAC FM es una emisora comunitaria sin √°nimo de lucro que emite desde A Coru√±a.\",\n",
    "    \"Naci√≥ en 1996 como una iniciativa de estudiantes universitarios de la Universidade da Coru√±a.\",\n",
    "    \"Es una radio libre, comunitaria y educativa, gestionada por voluntariado.\",\n",
    "    \"Tiene su sede en A Coru√±a, Galicia, Espa√±a.\",\n",
    "    \"CUAC significa 'Colectivo Universitario de Actividades Culturales'.\",\n",
    "\n",
    "    # üßë‚Äçü§ù‚Äçüßë Participaci√≥n y organizaci√≥n\n",
    "    \"Cualquier persona con inter√©s en la comunicaci√≥n y en aportar contenido social o cultural puede participar.\",\n",
    "    \"Cada programa es gestionado por su propio equipo, y CUAC proporciona el espacio, formaci√≥n y emisi√≥n.\",\n",
    "    \"El voluntariado es fundamental: todas las tareas de programaci√≥n, t√©cnica y gesti√≥n son realizadas por voluntarios.\",\n",
    "    \"CUAC ofrece talleres de formaci√≥n t√©cnica y de comunicaci√≥n radiof√≥nica a sus nuevos miembros.\",\n",
    "    \"Basta con presentar una propuesta al equipo de coordinaci√≥n y seguir unas pautas b√°sicas para comenzar a emitir.\",\n",
    "\n",
    "    # üó£Ô∏è Contenido y programaci√≥n\n",
    "    \"Emiten programas de m√∫sica, cultura, sociedad, pol√≠tica, feminismo, ecolog√≠a, etc.\",\n",
    "    \"Tratan temas sociales, culturales, educativos y comunitarios, especialmente los que no tienen cabida en medios comerciales.\",\n",
    "    \"La programaci√≥n se construye de forma colaborativa entre los diferentes programas y el equipo de coordinaci√≥n.\",\n",
    "    \"S√≠, CUAC da mucha visibilidad a artistas y grupos locales y emergentes.\",\n",
    "    \"Tiene una base estable de programas, pero tambi√©n permite rotaci√≥n y nuevos proyectos cada temporada.\",\n",
    "\n",
    "    # üéß Tecnolog√≠a y difusi√≥n\n",
    "    \"Emite en el 103.4 FM en A Coru√±a.\",\n",
    "    \"S√≠, tambi√©n puede escucharse en l√≠nea desde su p√°gina web y apps de radio por internet.\",\n",
    "    \"Utiliza equipos de grabaci√≥n digital, software de automatizaci√≥n y plataformas de streaming.\",\n",
    "    \"S√≠, muchos programas est√°n disponibles como podcast en su web y en plataformas como iVoox o Spotify.\",\n",
    "    \"En la p√°gina oficial de CUAC FM y en sus perfiles de podcasting.\",\n",
    "\n",
    "    # üì¢ Compromiso social y legal\n",
    "    \"Es un medio al servicio de la comunidad, ofreciendo una voz a colectivos y personas sin representaci√≥n en medios tradicionales.\",\n",
    "    \"Seg√∫n la legislaci√≥n espa√±ola, tiene derecho a emitir como medio comunitario, aunque la normativa a√∫n es limitada.\",\n",
    "    \"S√≠, CUAC FM ha denunciado en varias ocasiones la falta de reconocimiento legal y ha sufrido amenazas de cierre por falta de licencia.\",\n",
    "    \"Se financia a trav√©s de subvenciones, colaboraciones p√∫blicas, donaciones y autofinanciaci√≥n.\",\n",
    "    \"CUAC FM es miembro fundador de la Red de Medios Comunitarios (ReMC), con la que colabora activamente en iniciativas de defensa de los medios libres.\",\n",
    "\n",
    "    # üåç Impacto y proyecci√≥n\n",
    "    \"Tiene un papel activo en la comunidad coru√±esa, dando voz a causas sociales, iniciativas culturales y movimientos ciudadanos.\",\n",
    "    \"Organiza talleres, jornadas, encuentros de radios libres y actividades educativas en colegios e institutos.\",\n",
    "    \"S√≠, ha recibido reconocimientos por su trayectoria y por la promoci√≥n de la comunicaci√≥n libre y democr√°tica.\",\n",
    "    \"Colabora con radios libres de toda Espa√±a y Europa, as√≠ como con universidades, ONGs y colectivos sociales.\",\n",
    "    \"Enfrenta desaf√≠os como la falta de apoyo institucional, el acceso a licencias legales y la sostenibilidad econ√≥mica a largo plazo.\"\n",
    "]\n",
    "\n",
    "results_hybrid = []\n",
    "results_bm25 = []\n",
    "results_vector = []\n",
    "llm_answer_hybrid = []\n",
    "llm_answer_bm25 = []\n",
    "llm_answer_vector = []\n",
    "\n",
    "for question in cuac_fm_questions:\n",
    "    hybrid_result = retriever_hybrid(question)\n",
    "    bm25_result = retriever_bm25(question)\n",
    "    vector_result = retriever_vector(question)\n",
    "    results_hybrid.append(hybrid_result)\n",
    "    results_bm25.append(bm25_result)\n",
    "    results_vector.append(vector_result)\n",
    "    llm_answer_hybrid.append(chain.invoke({\"context\": hybrid_result, \"query\": question}))\n",
    "    llm_answer_bm25.append(chain.invoke({\"context\": bm25_result, \"query\": question}))\n",
    "    llm_answer_vector.append(chain.invoke({\"context\": vector_result, \"query\": question}))\n",
    "\n",
    "results = []\n",
    "index = 0\n",
    "for question in cuac_fm_questions:\n",
    "        print(index)\n",
    "        results.append({\n",
    "                \"question\": question,\n",
    "                \"retrieval_type\": \"hybrid\",\n",
    "                \"context\": results_hybrid[index],\n",
    "                \"answer\": llm_answer_hybrid[index],\n",
    "                \"ground_truth\": cuac_fm_answers[index]\n",
    "            })\n",
    "        results.append({\n",
    "                \"question\": question,\n",
    "                \"retrieval_type\": \"bm25\",\n",
    "                \"context\": results_bm25[index],\n",
    "                \"answer\": llm_answer_bm25[index],\n",
    "                \"ground_truth\": cuac_fm_answers[index]\n",
    "            })\n",
    "        results.append({\n",
    "                \"question\": question,\n",
    "                \"retrieval_type\": \"vector\",\n",
    "                \"context\": results_vector[index],\n",
    "                \"answer\": llm_answer_vector[index],\n",
    "                \"ground_truth\": cuac_fm_answers[index]\n",
    "            })\n",
    "        index += 1\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run RAGAS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from bert_score import score as bert_score\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from ragas.evaluation import EvaluationDataset\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj----\"\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "dataset = EvaluationDataset.from_pandas(df)\n",
    "\n",
    "# 6. Evaluate with RAGAS\n",
    "results_evaluation = evaluate(\n",
    "    dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store data frame into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_evaluation)\n",
    "\n",
    "# 7. Add RAGAS results_evaluation to DataFrame\n",
    "# Handle both dict and list output\n",
    "if hasattr(results_evaluation, 'scores'):\n",
    "    scores = results_evaluation.scores\n",
    "    if isinstance(scores, dict):\n",
    "        for metric_name, metric_scores in scores.items():\n",
    "            df[metric_name] = metric_scores\n",
    "    elif isinstance(scores, list):\n",
    "        # Try to convert to DataFrame and concat\n",
    "        scores_df = pd.DataFrame(scores)\n",
    "        df = pd.concat([df, scores_df], axis=1)\n",
    "\n",
    "# 8. Evaluate with BERTScore\n",
    "if 'user_input' in df.columns and 'reference' in df.columns:\n",
    "    P, R, F1 = bert_score(df[\"user_input\"].astype(str).tolist(), df[\"reference\"].astype(str).tolist(), lang=\"es\")\n",
    "    df[\"bertscore_f1\"] = F1.tolist()\n",
    "\n",
    "# 9. Show all metrics per question and retrieval type (if present)\n",
    "cols_to_show = [c for c in [\"user_input\", \"retrieval_type\", \"faithfulness\", \"answer_relevancy\", \"context_precision\", \"context_recall\", \"bertscore_f1\"] if c in df.columns]\n",
    "print(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store evaluation into CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_bm25 = pd.read_csv('output_bm25.csv')\n",
    "df_vector = pd.read_csv('output_vector.csv')\n",
    "df_hybrid = pd.read_csv('output_hybrid.csv')\n",
    "\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall', 'bertscore_f1']\n",
    "\n",
    "print(\"BM25 Means:\")\n",
    "print(df_bm25[metrics].mean())\n",
    "print(\"\\nVector Means:\")\n",
    "print(df_vector[metrics].mean())\n",
    "print(\"\\nHybrid Means:\")\n",
    "print(df_hybrid[metrics].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
